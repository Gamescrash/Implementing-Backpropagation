{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6bb7976",
   "metadata": {},
   "source": [
    "# Neural Network Autoencoder Homework\n",
    "\n",
    "This notebook serves as the skeleton for implementing an 8‑3‑8 autoencoder from scratch using NumPy. Each section corresponds to a step in building, training, and analyzing the network. Fill in the code cells with your implementation and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c809ae",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal is to implement and train a neural network with one hidden layer to reproduce the input one‑hot vectors. The network architecture consists of:\n",
    "\n",
    "- An input layer of 8 units.\n",
    "- A hidden layer with 3 units (plus a bias).\n",
    "- An output layer of 8 units.\n",
    "\n",
    "The tasks include:\n",
    "\n",
    "1. Generating the dataset of one‑hot vectors.\n",
    "2. Initializing network parameters.\n",
    "3. Implementing activation functions, loss, and backpropagation.\n",
    "4. Training the network using gradient descent.\n",
    "5. Monitoring learning performance and plotting the loss curve.\n",
    "6. Analyzing the hidden layer activations and learned weights.\n",
    "\n",
    "Follow the sections below to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f0064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducibility (optional)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56954899",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "Create the dataset consisting of the 8 one‑hot vectors. Each example has 7 zeros and 1 one. The output you aim for is the same as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 8 one‑hot vectors as input and target data\n",
    "X = np.eye(8)  # shape (8, 8)\n",
    "Y = X.copy()    # same as input\n",
    "\n",
    "print('Input dataset X:')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3ad82",
   "metadata": {},
   "source": [
    "## Initialize Parameters\n",
    "\n",
    "Initialize the weights and biases for the network. Use a suitable initialization strategy (e.g., Xavier/Glorot initialization) to help the network converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e796c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "# W1: hidden layer weights (shape 3×8), b1: hidden layer biases (shape 3×1)\n",
    "# W2: output layer weights (shape 8×3), b2: output layer biases (shape 8×1)\n",
    "\n",
    "input_size = 8\n",
    "hidden_size = 3\n",
    "output_size = 8\n",
    "\n",
    "# Xavier/Glorot initialization for sigmoid activation\n",
    "limit_W1 = np.sqrt(6 / (input_size + hidden_size))\n",
    "limit_W2 = np.sqrt(6 / (hidden_size + output_size))\n",
    "\n",
    "W1 = np.random.uniform(-limit_W1, limit_W1, (hidden_size, input_size))\n",
    "b1 = np.zeros((hidden_size, 1))\n",
    "W2 = np.random.uniform(-limit_W2, limit_W2, (output_size, hidden_size))\n",
    "b2 = np.zeros((output_size, 1))\n",
    "\n",
    "# Print shapes to verify\n",
    "print('W1 shape:', W1.shape)\n",
    "print('b1 shape:', b1.shape)\n",
    "print('W2 shape:', W2.shape)\n",
    "print('b2 shape:', b2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868927d",
   "metadata": {},
   "source": [
    "## Activation and Loss Functions\n",
    "\n",
    "Implement the sigmoid activation function and the binary cross‑entropy loss. These will be used in the forward pass and the computation of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08245dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Binary cross‑entropy loss (averaged over outputs and samples)\n",
    "def bce_loss(y, y_hat):\n",
    "    # Add a small epsilon to avoid log(0)\n",
    "    eps = 1e-12\n",
    "    y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# Example: test activation and loss\n",
    "z_example = np.array([[0], [1], [-1]])\n",
    "a_example = sigmoid(z_example)\n",
    "print('Sigmoid example:', a_example.flatten())\n",
    "\n",
    "y_true = np.array([[1, 0], [0, 1]])\n",
    "y_pred = np.array([[0.9, 0.1], [0.1, 0.9]])\n",
    "print('BCE loss example:', bce_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfba46e",
   "metadata": {},
   "source": [
    "## Forward and Backpropagation\n",
    "\n",
    "Implement the forward pass to compute the network outputs given the inputs. Then implement backpropagation to compute gradients of the loss with respect to weights and biases. Use vectorized operations for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    # X: shape (8, N) or (N, 8) depending on representation\n",
    "    # Convert X to shape (input_size, N) for consistency\n",
    "    if X.shape[0] != input_size:\n",
    "        X = X.T\n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    Y_hat = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, Y_hat\n",
    "\n",
    "# Backpropagation\n",
    "def backpropagation(X, Y, Z1, A1, Y_hat, W2):\n",
    "    N = X.shape[1]  # number of samples\n",
    "    \n",
    "    # Compute output error\n",
    "    dZ2 = (Y_hat - Y) / (output_size * N)  # BCE derivative and average\n",
    "    dW2 = dZ2 @ A1.T\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute hidden error\n",
    "    dA1 = W2.T @ dZ2\n",
    "    dZ1 = dA1 * A1 * (1 - A1)\n",
    "    dW1 = dZ1 @ X.T\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Example usage: do one forward and backprop step\n",
    "Z1, A1, Z2, Y_hat = forward_pass(X, W1, b1, W2, b2)\n",
    "dW1, db1_grad, dW2, db2_grad = backpropagation(X.T, Y.T, Z1, A1, Y_hat, W2)\n",
    "print('dW1 shape:', dW1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14938b2",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Implement the gradient descent loop to update parameters until the network converges. Monitor the loss at each epoch and stop when a desired threshold is reached or after a fixed number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "learning_rate = 0.5\n",
    "max_epochs = 5000\n",
    "loss_history = []\n",
    "\n",
    "# Transpose data for easier math (columns are samples)\n",
    "X_train = X.T  # shape (8, 8)\n",
    "Y_train = Y.T  # shape (8, 8)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Forward pass\n",
    "    Z1, A1, Z2, Y_hat = forward_pass(X_train, W1, b1, W2, b2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = bce_loss(Y_train, Y_hat)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backpropagation\n",
    "    dW1, db1_grad, dW2, db2_grad = backpropagation(X_train, Y_train, Z1, A1, Y_hat, W2)\n",
    "    \n",
    "    # Update parameters\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2_grad\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1_grad\n",
    "    \n",
    "    # Check convergence criterion (e.g., loss threshold)\n",
    "    if loss < 1e-3:\n",
    "        print(f'Early stopping at epoch {epoch+1} with loss {loss:.6f}')\n",
    "        break\n",
    "\n",
    "print('Final loss:', loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db3a51",
   "metadata": {},
   "source": [
    "## Plot Loss Curve\n",
    "\n",
    "Visualize the decrease in loss over training epochs to observe convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Cross‑Entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d3617",
   "metadata": {},
   "source": [
    "## Evaluate and Interpret the Hidden Layer\n",
    "\n",
    "After training, evaluate the network on all input vectors and analyze the activations of the hidden layer. Compare the raw activations and thresholded values (e.g., > 0.5) to see if the network has learned a binary encoding for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hidden activations and outputs\n",
    "Z1, A1, Z2, Y_hat_final = forward_pass(X_train, W1, b1, W2, b2)\n",
    "\n",
    "# Hidden activations for each input\n",
    "print('Hidden activations (raw):')\n",
    "print(A1)\n",
    "\n",
    "# Threshold the activations to form a binary code\n",
    "hidden_codes = (A1 > 0.5).astype(int)\n",
    "print('Hidden activations (thresholded > 0.5):')\n",
    "print(hidden_codes)\n",
    "\n",
    "# Compare final outputs to targets\n",
    "print('Final outputs:')\n",
    "print(np.round(Y_hat_final, 3))\n",
    "print('Targets:')\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f683e",
   "metadata": {},
   "source": [
    "## Discussion and Analysis\n",
    "\n",
    "In this section, discuss the performance of your network:\n",
    "\n",
    "- How many epochs were required for convergence?\n",
    "- How did the choice of learning rate affect convergence?\n",
    "- Did different random initializations influence convergence time or final performance?\n",
    "- What patterns do you observe in the hidden activations?\n",
    "- Does the network learn a form of binary encoding?\n",
    "\n",
    "Provide insights and observations based on your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef73c2",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Summarize your findings, including the effectiveness of the autoencoder in reconstructing the input patterns, and reflect on how well the hidden layer captures the underlying structure of the data. Discuss any improvements or further experiments you would like to perform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
